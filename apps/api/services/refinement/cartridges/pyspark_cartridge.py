from pathlib import Path
from typing import Dict, Any, List
from .base_cartridge import Cartridge

class PySparkCartridge(Cartridge):
    """
    Generates PySpark/Delta Lake code for Medallion Architecture.
    """

    def get_file_extension(self) -> str:
        return ".py"

    def generate_scaffolding(self) -> Dict[str, str]:
        """Generates config.py and utils.py"""
        naming = self.registry.get("naming", {})
        paths = self.registry.get("paths", {})
        
        s_bronze = naming.get("bronze_schema", "bronze_raw")
        s_silver = naming.get("silver_schema", "silver_curated")
        s_gold = naming.get("gold_schema", "gold_business")
        
        p_bronze = paths.get("bronze_path", "/mnt/datalake/bronze")
        p_silver = paths.get("silver_path", "/mnt/datalake/silver")
        p_gold = paths.get("gold_path", "/mnt/datalake/gold")
        
        config_content = f"""# Medallion Architecture Configuration
import os

class Config:
    CATALOG = "main_catalog"
    
    # Schemas
    SCHEMA_BRONZE = "{s_bronze}"
    SCHEMA_SILVER = "{s_silver}"
    SCHEMA_GOLD = "{s_gold}"
    
    # Paths (if using external locations)
    PATH_BRONZE = "{p_bronze}"
    PATH_SILVER = "{p_silver}"
    PATH_GOLD = "{p_gold}"
    
    @staticmethod
    def get_jdbc_url(secret_scope, secret_key):
        return dbutils.secrets.get(scope=secret_scope, key=secret_key)
"""

        utils_content = """# Common Utility Functions
from pyspark.sql.functions import current_timestamp, lit

def add_ingestion_metadata(df):
    return df.withColumn("_ingestion_timestamp", current_timestamp()) \\
             .withColumn("_source_system", lit("SSIS_MIGRATION"))

def quarantine_bad_records(df, rules, quarantine_table):
    # Simplified quarantine logic
    pass
"""
        return {
            "config.py": config_content,
            "utils.py": utils_content
        }

    def generate_bronze(self, table_metadata: Dict[str, Any]) -> str:
        source_path = Path(table_metadata.get("source_path", "unknown_source.py"))
        original_code = table_metadata.get("original_code", "")
        
        # Disable original writes
        processed_lines = []
        for line in original_code.splitlines():
            if any(x in line for x in [".write", ".save", "saveAsTable", "display(", "OPTIMIZE", "VACUUM"]):
                processed_lines.append(f"# [DISABLED_BY_ARCHITECT] {line}")
            else:
                processed_lines.append(line)
        original_code_safe = "\n".join(processed_lines)
        
        return f"""# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: {source_path.name}

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
{original_code_safe}

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{{Config.CATALOG}}.{{Config.SCHEMA_BRONZE}}.{source_path.stem}"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
"""

    def generate_silver(self, table_metadata: Dict[str, Any]) -> str:
        source_path = Path(table_metadata.get("source_path", "unknown.py"))
        output_table_name = table_metadata.get("output_table_name", source_path.stem)
        pk_columns = table_metadata.get("pk_columns", ["id"])
        
        merge_condition = " AND ".join([f"target.{pk} = source.{pk}" for pk in pk_columns])
        pk_display = ", ".join(pk_columns)
        
        silver_prefix = self.registry.get("naming", {}).get("silver_prefix", "stg_")

        return f"""# SILVER LAYER TRANSFORMATION
# Generated by Shift-T Architect Agent
# Source: {source_path.name}
# Primary Key(s): {pk_display}

from config import Config
from delta.tables import *
from pyspark.sql.functions import *

# Read Bronze
df_bronze = spark.read.table(f"{{Config.CATALOG}}.{{Config.SCHEMA_BRONZE}}.{source_path.stem}")

# Deduplicate
df_clean = df_bronze.dropDuplicates({pk_columns})

# SCD Type 2 Merge (Upsert)
target_table_name = f"{{Config.CATALOG}}.{{Config.SCHEMA_SILVER}}.{output_table_name}"

if SparkSession.active.catalog.tableExists(target_table_name) and "{pk_columns[0]}" != "id":
    target_table = DeltaTable.forName(spark, target_table_name)
    target_table.alias("target").merge(
        df_clean.alias("source"),
        "{merge_condition}"
    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
else:
    df_clean.write.format("delta").mode("overwrite").saveAsTable(target_table_name)
"""

    def generate_gold(self, table_metadata: Dict[str, Any]) -> str:
        source_path = Path(table_metadata.get("source_path", "unknown.py"))
        output_table_name = table_metadata.get("output_table_name", source_path.stem)
        table_type = table_metadata.get("table_type", "DIMENSION")
        
        logic_comment = "# Gold Logic: Business-ready projection."
        select_logic = "df_silver.select(\"*\")"
        if table_type == "FACT":
            logic_comment = "# Gold Logic: Fact table aggregation / Semantic measures."
            select_logic = "df_silver.select(\"*\", (col(\"qty\") * col(\"unitprice\")).alias(\"total_amount\"))"

        return f"""# GOLD LAYER - BUSINESS VIEW ({table_type})
# Generated by Shift-T Architect Agent
# Source: {source_path.name}

from config import Config
from pyspark.sql.functions import *

target_silver_table = f"{{Config.CATALOG}}.{{Config.SCHEMA_SILVER}}.{source_path.stem}" 
df_silver = spark.read.table(target_silver_table)

{logic_comment}
df_gold = {select_logic}

# Write to Gold
target_gold_table = f"{{Config.CATALOG}}.{{Config.SCHEMA_GOLD}}.{output_table_name}"
df_gold.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_gold_table)

print(f"Gold Layer updated: {{target_gold_table}}")
"""
