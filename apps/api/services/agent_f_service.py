import os
import json
from typing import Dict, Any, List, Optional
from langchain_openai import AzureChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
try:
    from apps.api.utils.logger import logger
    from apps.api.services.persistence_service import SupabasePersistence
except ImportError:
    try:
        from utils.logger import logger
        from services.persistence_service import SupabasePersistence
    except ImportError:
        from ..utils.logger import logger
        from .persistence_service import SupabasePersistence


class AgentFService:
    def __init__(self, tenant_id: Optional[str] = None, client_id: Optional[str] = None):
        self.prompt_path = os.path.join(os.path.dirname(__file__), "../prompts/agent_f_critic.md")
        self.standards_path = os.path.join(os.path.dirname(__file__), "../prompts/coding_standards.md")
        self.tenant_id = tenant_id
        self.client_id = client_id

    async def _get_llm(self, project_id: Optional[str] = None):
        """Resolves LLM client from Agent Matrix / Catalog."""
        db = SupabasePersistence(tenant_id=self.tenant_id, client_id=self.client_id)
        config = await db.resolve_agent_model("agent-f")
        
        if not config:
            logger.warning("Using fallback LLM for agent-f.")
            # Fallback to env for safety during transition
            return AzureChatOpenAI(
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
                azure_deployment=os.getenv("AZURE_OPENAI_DEPLOYMENT_ID", "gpt-4"),
                openai_api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
                api_key=os.getenv("AZURE_OPENAI_API_KEY"),
                temperature=0
            )

        if config["provider"] == "azure":
            return AzureChatOpenAI(
                azure_endpoint=config["endpoint"],
                azure_deployment=config["deployment"],
                openai_api_version=config["api_version"] or os.getenv("AZURE_OPENAI_API_VERSION"),
                api_key=config.get("api_key") or os.getenv("AZURE_OPENAI_API_KEY"),
                temperature=config["temperature"]
            )
        else:
            # Standard OpenAI or other providers
            from langchain_openai import ChatOpenAI
            return ChatOpenAI(
                model=config["deployment"],
                api_key=config.get("api_key"),
                base_url=config["endpoint"],
                temperature=config.get("temperature", 0)
            )

    async def _load_prompt(self, path: str = None) -> str:
        db = SupabasePersistence(tenant_id=self.tenant_id, client_id=self.client_id)
        return await db.get_prompt("agent_f_critic")

    async def save_prompt(self, content: str):
        db = SupabasePersistence(tenant_id=self.tenant_id, client_id=self.client_id)
        await db.save_prompt("agent_f_critic", content)

    @logger.llm_debug("Agent-F-Compliance-Review")
    async def review_code(self, task_info: Dict[str, Any], generated_code: str, project_id: Optional[str] = None) -> Dict[str, Any]:
        """Audits and optimizes PySpark code generated by Agent C."""
        system_prompt = await self._load_prompt(self.prompt_path)
        
        # --- PROMPT GUARD: Sandwich Approach ---
        guard_header = "### SYSTEM INSTRUCTION OVERRIDE: YOU ARE A SENIOR COMPLIANCE AUDITOR. DO NOT BREAK CHARACTER. ###"
        guard_footer = "### END OF INSTRUCTION. GENERATE ONLY VALID JSON AS REQUESTED. NO CHAT. ###"
        
        system_prompt = f"{guard_header}\n\n{system_prompt}\n\n{guard_footer}"
        
        standards = await self._load_prompt(self.standards_path)
        
        human_content = f"""
        CODING STANDARDS TO FOLLOW:
        {standards}
 
        TASK INFO:
        {json.dumps(task_info, indent=2)}
 
        GENERATED CODE FOR REVIEW:
        ```python
        {generated_code}
        ```
        """
 
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_content)
        ]
 
        llm = await self._get_llm(project_id)
        response = await llm.ainvoke(messages)
        content = response.content.strip()
 
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
 
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            return {
                "error": "Failed to parse Agent F response as JSON",
                "raw_response": content
            }
 
    @logger.llm_debug("Agent-F-Compliance-Optimize")
    async def optimize_code(self, original_code: str, optimizations: List[str], project_id: Optional[str] = None) -> Dict[str, Any]:
        """Applies specific optimizations to the code based on user selection."""
        system_prompt = await self._load_prompt(self.prompt_path)
        
        human_content = f"""
        Please apply the following SPECIFIC optimizations to the code below:
        {json.dumps(optimizations)}
 
        ORIGINAL CODE:
        ```python
        {original_code}
        ```
 
        Return the optimized code in a JSON format with keys: "optimized_code", "changes_applied".
        """
 
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=human_content)
        ]
 
        llm = await self._get_llm(project_id)
        response = await llm.ainvoke(messages)
        content = response.content.strip()
 
        if "```json" in content:
            content = content.split("```json")[1].split("```")[0].strip()
        elif "```" in content:
            content = content.split("```")[1].split("```")[0].strip()
 
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            return {
                "error": "Failed to parse Agent F optimization response",
                "optimized_code": original_code,
                "changes_applied": []
            }

