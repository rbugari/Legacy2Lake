# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: DimCategory.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: DimCategory.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Source (JDBC)
# Extract SQL from input
sql_query = '''SELECT categoryid,categoryname FROM Production.Categories WHERE categoryid > ?'''
# Parameterize the '?' value using a widget or parameter (default to 0)
categoryid_min = dbutils.widgets.get('categoryid_min') if dbutils.widgets.get('categoryid_min', None) else '0'
sql_query_param = sql_query.replace('?', categoryid_min)

# JDBC connection details (use secrets, never hardcode)
db_url = dbutils.secrets.get('jdbc_scope', 'jdbc_url')
db_user = dbutils.secrets.get('jdbc_scope', 'jdbc_user')
db_password = dbutils.secrets.get('jdbc_scope', 'jdbc_password')

jdbc_options = {
    "url": db_url,
    "user": db_user,
    "password": db_password,
    "dbtable": f"({sql_query_param}) as src"
}
df_source = spark.read.format("jdbc").options(**jdbc_options).load()

# 3. Transformations (Apply Logic)
# No lookups or complex logic for this dimension

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# For DimCategory, surrogate key is typically 'CategoryKey', business key is 'categoryid'
target_table_name = "DimCategory"
bk_cols = ["categoryid"]
sk_col = "CategoryKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = df_source.join(df_target, on=bk_cols, how="left")
else:
    df_joined = df_source.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
from functools import reduce
df_with_sk = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    # Define unknown row schema explicitly
    unknown_row = {
        "categoryid": -1,
        "categoryname": "Unknown",
        "CategoryKey": -1
    }
    # Check if unknown exists
    if df.filter(col("CategoryKey") == -1).count() == 0:
        unknown_df = spark.createDataFrame([unknown_row], df.schema)
        df = df.unionByName(unknown_df)
    return df

df_with_sk = ensure_unknown_member(df_with_sk)

# 4. Mandatory Type Casting (STRICT)
# Target schema: categoryid INTEGER, categoryname STRING, CategoryKey INTEGER
# (If schema is missing, infer from SSIS types and platform rules)
df_final = df_with_sk \
    .withColumn("categoryid", col("categoryid").cast("integer")) \
    .withColumn("categoryname", col("categoryname").cast("string")) \
    .withColumn("CategoryKey", col("CategoryKey").cast("integer"))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite/merge for idempotency
# Use Delta Lake upsert (MERGE) pattern

deltaTable = DeltaTable.forName(spark, target_table_name)
deltaTable.alias("target").merge(
    df_final.alias("source"),
    "target.categoryid = source.categoryid"
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()

# Optimization: Z-ORDER on business key
# [DISABLED_BY_ARCHITECT] spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (categoryid)")

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.DimCategory"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
