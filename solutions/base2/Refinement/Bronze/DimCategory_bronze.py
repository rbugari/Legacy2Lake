# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: DimCategory.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: DimCategory.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Source (JDBC)
# Extract credentials securely
jdbc_hostname = dbutils.secrets.get('jdbc_scope', 'prod_server')
jdbc_port = dbutils.secrets.get('jdbc_scope', 'prod_port')
jdbc_database = dbutils.secrets.get('jdbc_scope', 'prod_db')
jdbc_username = dbutils.secrets.get('jdbc_scope', 'prod_user')
jdbc_password = dbutils.secrets.get('jdbc_scope', 'prod_pwd')
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={jdbc_database}"

# Parameter for categoryid > ? (Assume widget or parameter)
categoryid_min = int(dbutils.widgets.get('categoryid_min'))

sql_query = f"SELECT categoryid, categoryname FROM Production.Categories WHERE categoryid > {categoryid_min}"

# Read source table
source_df = (
    spark.read.format("jdbc")
    .option("url", jdbc_url)
    .option("dbtable", f"({sql_query}) as src")
    .option("user", jdbc_username)
    .option("password", jdbc_password)
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (No lookups, direct mapping)
# Target schema is not provided, so we infer from SSIS and platform rules
# Assume DimCategory: [CategorySK (INTEGER, surrogate key), CategoryID (INTEGER), CategoryName (STRING)]

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
target_table_name = "base2.dbo.DimCategory"
bk_cols = ["categoryid"]
sk_col = "CategorySK"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = source_df.join(df_target, on=bk_cols, how="left")
else:
    df_joined = source_df.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
category_df = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    unknown_row = [( -1, -1, "Unknown" )]
    unknown_schema = StructType([
        StructField("CategorySK", IntegerType(), False),
        StructField("categoryid", IntegerType(), False),
        StructField("categoryname", StringType(), False)
    ])
    df_unknown = spark.createDataFrame(unknown_row, unknown_schema)
    # Check if unknown exists
    if df.filter(col("CategorySK") == -1).count() == 0:
        return df.unionByName(df_unknown)
    else:
        return df

category_df = ensure_unknown_member(category_df)

# 4. Mandatory Type Casting (STRICT)
category_df = (
    category_df
    .withColumn("CategorySK", col("CategorySK").cast("integer"))
    .withColumn("categoryid", col("categoryid").cast("integer"))
    .withColumn("categoryname", col("categoryname").cast("string"))
)

# 5. Writing to Silver/Gold (Idempotent Overwrite)
# Overwrite the table (idempotent for dimensions)
# [DISABLED_BY_ARCHITECT] category_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").saveAsTable(target_table_name)

# 6. Optimization (Z-ORDER on Business Key)
deltaTable = DeltaTable.forName(spark, target_table_name)
deltaTable.optimize().zorderBy("categoryid")

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.DimCategory"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
