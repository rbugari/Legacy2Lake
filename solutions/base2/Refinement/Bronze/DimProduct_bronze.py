# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: DimProduct.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: DimProduct.dtsx Migration
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Source (Bronze Layer)
# Use JDBC to read from SQL Server (Production.Products)
# Credentials and connection details should be parameterized and retrieved securely
jdbc_hostname = dbutils.widgets.get("jdbc_hostname")
jdbc_port = dbutils.widgets.get("jdbc_port")
jdbc_database = dbutils.widgets.get("jdbc_database")
jdbc_username = dbutils.secrets.get(scope="sql_scope", key="sql_user")
jdbc_password = dbutils.secrets.get(scope="sql_scope", key="sql_password")
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={jdbc_database}"

source_query = "SELECT * FROM Production.Products"
df_source = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", f"({source_query}) as src") \
    .option("user", jdbc_username) \
    .option("password", jdbc_password) \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .load()

# 3. Transformations (Apply Logic)
# No lookups specified. Direct mapping from source to target.

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "dbo.DimProduct"
bk_cols = ["ProductID"]  # Assuming ProductID is the business key
sk_col = "DimProductKey"  # Surrogate Key for dimension

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target:
    df_joined = df_source.join(df_target, on=bk_cols, how="left")
else:
    df_joined = df_source.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
from pyspark.sql import DataFrame

def unionByName_safe(df1: DataFrame, df2: DataFrame) -> DataFrame:
    # Ensure both DataFrames have the same columns in the same order
    cols = [c for c in df1.columns if c in df2.columns]
    return df1.select(*cols).unionByName(df2.select(*cols))

df_with_sk = unionByName_safe(df_existing, df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    # Define the schema for the unknown member
    unknown_row = {
        sk_col: -1,
        "ProductID": -1,
        "ProductName": "Unknown",
        "ProductNumber": "Unknown",
        "Color": "Unknown",
        "StandardCost": 0.0,
        "ListPrice": 0.0,
        "Size": "Unknown",
        "Weight": 0.0,
        "ProductCategoryID": -1,
        "ProductModelID": -1,
        "SellStartDate": None,
        "SellEndDate": None,
        "DiscontinuedDate": None,
        "rowguid": "00000000-0000-0000-0000-000000000000",
        "ModifiedDate": None
    }
    # Check if unknown member exists
    if df.filter(col(sk_col) == -1).count() == 0:
        unknown_df = spark.createDataFrame([unknown_row])
        df = df.unionByName(unknown_df)
    return df

df_with_sk = ensure_unknown_member(df_with_sk)

# 4. Mandatory Type Casting (STRICT)
# Define the target schema explicitly (example based on AdventureWorks)
target_schema = [
    {"name": "DimProductKey", "type": "INTEGER"},
    {"name": "ProductID", "type": "INTEGER"},
    {"name": "ProductName", "type": "STRING"},
    {"name": "ProductNumber", "type": "STRING"},
    {"name": "Color", "type": "STRING"},
    {"name": "StandardCost", "type": "DECIMAL(19,4)"},
    {"name": "ListPrice", "type": "DECIMAL(19,4)"},
    {"name": "Size", "type": "STRING"},
    {"name": "Weight", "type": "DECIMAL(19,4)"},
    {"name": "ProductCategoryID", "type": "INTEGER"},
    {"name": "ProductModelID", "type": "INTEGER"},
    {"name": "SellStartDate", "type": "TIMESTAMP"},
    {"name": "SellEndDate", "type": "TIMESTAMP"},
    {"name": "DiscontinuedDate", "type": "TIMESTAMP"},
    {"name": "rowguid", "type": "STRING"},
    {"name": "ModifiedDate", "type": "TIMESTAMP"}
]

for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in df_with_sk.columns:
        df_with_sk = df_with_sk.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite/merge into DimProduct dimension table
# Use Delta Lake for idempotency

delta_table_path = f"/mnt/delta/{target_table_name.replace('.', '/')}"

# If table exists, overwrite; else, create
if DeltaTable.isDeltaTable(spark, delta_table_path):
# [DISABLED_BY_ARCHITECT]     df_with_sk.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(delta_table_path)
else:
# [DISABLED_BY_ARCHITECT]     df_with_sk.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(delta_table_path)

# Optional: Z-ORDER optimization on ProductID
# [DISABLED_BY_ARCHITECT] spark.sql(f"OPTIMIZE delta.`{delta_table_path}` ZORDER BY (ProductID)")

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.DimProduct"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
