# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: FactSales.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: FactSales.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Source Inputs (Explicit JDBC)
# Input 1: QUERY: SELECT T.* FROM tempStage T INNER JOIN dw..V_DimProduct P ON P.productid = T.productid;
db_url_1 = dbutils.secrets.get('jdbc', 'dw_url')
db_user_1 = dbutils.secrets.get('jdbc', 'dw_user')
db_password_1 = dbutils.secrets.get('jdbc', 'dw_password')
sql_query_1 = '''SELECT T.* FROM tempStage T INNER JOIN dw..V_DimProduct P ON P.productid = T.productid'''
df_tempStage = spark.read.format('jdbc') \
    .option('url', db_url_1) \
    .option('user', db_user_1) \
    .option('password', db_password_1) \
    .option('dbtable', f'({sql_query_1}) as src') \
    .load()

# Input 2: QUERY: select O.orderid,O.custid,O.empid,O.shipperid,P.categoryid,P.supplierid,OD.qty,OD.unitprice,OD.discount,OD.productid from Sales.orders O  INNER JOIN Sales.OrderDetails OD ON O.orderid =OD.orderid inner join Production.Products P ON P.productid=OD.productid

# Assume connection info for Sales DB is in secrets as 'sales_url', 'sales_user', 'sales_password'
db_url_2 = dbutils.secrets.get('jdbc', 'sales_url')
db_user_2 = dbutils.secrets.get('jdbc', 'sales_user')
db_password_2 = dbutils.secrets.get('jdbc', 'sales_password')
sql_query_2 = '''select O.orderid,O.custid,O.empid,O.shipperid,P.categoryid,P.supplierid,OD.qty,OD.unitprice,OD.discount,OD.productid from Sales.orders O  INNER JOIN Sales.OrderDetails OD ON O.orderid =OD.orderid inner join Production.Products P ON P.productid=OD.productid'''
df_orders = spark.read.format('jdbc') \
    .option('url', db_url_2) \
    .option('user', db_user_2) \
    .option('password', db_password_2) \
    .option('dbtable', f'({sql_query_2}) as src') \
    .load()

# 3. Transformations (Apply Logic)
# For FactSales, join tempStage with orders data on productid (assuming this is the business key)
df_fact_sales_stage = df_tempStage.join(df_orders, on=['productid'], how='inner')

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = 'base2.dbo.FactSales'
bk_cols = ['orderid', 'productid']  # Assuming composite business key
sk_col = 'FactSalesSK'  # Surrogate Key column name

try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except:
    df_target = None
    max_sk = 0

if df_target:
    df_joined = df_fact_sales_stage.join(df_target, on=bk_cols, how='left')
else:
    df_joined = df_fact_sales_stage.withColumn(sk_col, lit(None).cast('integer'))

window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)
df_with_sk = df_existing.unionByName(df_new)

# 4. Mandatory Type Casting (STRICT)
# NOTE: Target schema is missing in input, so we cannot cast columns. If schema is provided, apply strict casting here.
# Example:
# for field in target_schema:
#     col_name = field["name"]
#     target_type = field["type"]
#     if col_name in df_with_sk.columns:
#         df_with_sk = df_with_sk.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Idempotent write: Overwrite partition or use MERGE if SCD logic is required. For Fact table, overwrite is typical.
# [DISABLED_BY_ARCHITECT] df_with_sk.write.format('delta').mode('overwrite').option('overwriteSchema', 'true').saveAsTable(target_table_name)

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.FactSales"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
