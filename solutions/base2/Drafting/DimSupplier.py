# COMMAND ----------
# Title: DimSupplier Dimension Load
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Source (JDBC)
# Extract credentials securely
jdbc_hostname = dbutils.secrets.get(scope="sql_server_secrets", key="hostname")
jdbc_port = dbutils.secrets.get(scope="sql_server_secrets", key="port")
jdbc_database = dbutils.secrets.get(scope="sql_server_secrets", key="database")
jdbc_username = dbutils.secrets.get(scope="sql_server_secrets", key="username")
jdbc_password = dbutils.secrets.get(scope="sql_server_secrets", key="password")
jdbc_url = f"jdbc:sqlserver://{jdbc_hostname}:{jdbc_port};databaseName={jdbc_database}"

# Parameter for supplierid threshold (replace with widget or parameter as needed)
supplierid_threshold = dbutils.widgets.get("supplierid_threshold") if dbutils.widgets.get("supplierid_threshold") else 0

source_query = f"SELECT supplierid, companyname, address, postalcode, phone, city, country FROM Production.Suppliers WHERE supplierid > {supplierid_threshold}"

# Read source data
source_df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", f"({source_query}) as src") \
    .option("user", jdbc_username) \
    .option("password", jdbc_password) \
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver") \
    .load()

# 3. Transformations (Apply Logic)
# No lookups in this task. All columns are direct mappings.

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "dbo.DimSupplier"
bk_cols = ["supplierid"]
sk_col = "SupplierKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target:
    df_joined = source_df.join(df_target, on=bk_cols, how="left")
else:
    df_joined = source_df.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
supplier_df = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    unknown_row = {
        "SupplierKey": -1,
        "supplierid": -1,
        "companyname": "Unknown",
        "address": "Unknown",
        "postalcode": "Unknown",
        "phone": "Unknown",
        "city": "Unknown",
        "country": "Unknown"
    }
    # Check if unknown exists
    if df.filter(col("SupplierKey") == -1).count() == 0:
        unknown_df = spark.createDataFrame([unknown_row])
        df = df.unionByName(unknown_df)
    return df

supplier_df = ensure_unknown_member(supplier_df)

# 4. Mandatory Type Casting (STRICT)
# Define target schema explicitly (as per platform rules)
target_schema = [
    {"name": "SupplierKey", "type": "INTEGER"},
    {"name": "supplierid", "type": "INTEGER"},
    {"name": "companyname", "type": "STRING"},
    {"name": "address", "type": "STRING"},
    {"name": "postalcode", "type": "STRING"},
    {"name": "phone", "type": "STRING"},
    {"name": "city", "type": "STRING"},
    {"name": "country", "type": "STRING"}
]
for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in supplier_df.columns:
        supplier_df = supplier_df.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite/merge for idempotency
# Use Delta Lake for dimension table
supplier_delta = DeltaTable.createIfNotExists(spark) \
    .tableName(target_table_name) \
    .addColumn("SupplierKey", IntegerType()) \
    .addColumn("supplierid", IntegerType()) \
    .addColumn("companyname", StringType()) \
    .addColumn("address", StringType()) \
    .addColumn("postalcode", StringType()) \
    .addColumn("phone", StringType()) \
    .addColumn("city", StringType()) \
    .addColumn("country", StringType()) \
    .property("delta.enableChangeDataFeed", "true") \
    .location(f"/mnt/delta/{target_table_name}") \
    .execute()

# Upsert logic (idempotent)
if DeltaTable.isDeltaTable(spark, f"/mnt/delta/{target_table_name}"):
    delta_table = DeltaTable.forPath(spark, f"/mnt/delta/{target_table_name}")
    delta_table.alias("target").merge(
        supplier_df.alias("source"),
        "target.supplierid = source.supplierid"
    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()
else:
    supplier_df.write.format("delta").mode("overwrite").option("overwriteSchema", "true").save(f"/mnt/delta/{target_table_name}")

# Optional: Z-ORDER optimization
spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (supplierid)")
