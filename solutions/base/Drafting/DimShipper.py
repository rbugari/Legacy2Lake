# COMMAND ----------
# Title: DimShipper.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Bronze / Source
# Extract JDBC connection details securely
jdbc_url = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-url")
jdbc_user = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-user")
jdbc_password = dbutils.secrets.get(scope="jdbc-secrets", key="sales-db-password")

# Parameter for shipperid threshold (replace with widget or parameter as needed)
shipperid_threshold = dbutils.widgets.get("shipperid_threshold")

source_query = f"SELECT * FROM Sales.Shippers WHERE shipperid > {shipperid_threshold}"
df_source = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("user", jdbc_user) \
    .option("password", jdbc_password) \
    .option("dbtable", f"({source_query}) as src") \
    .load()

# 3. Transformations (Apply Logic)
# No lookups or complex transformations required for this task.

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "DimShipper"
bk_cols = ["shipperid"]  # Business Key column from source
sk_col = "ShipperKey"    # Surrogate Key column in target

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = df_source.join(df_target, on=bk_cols, how="left")
else:
    df_joined = df_source.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
from pyspark.sql import DataFrame

def union_by_name(df1: DataFrame, df2: DataFrame) -> DataFrame:
    # Ensure both DataFrames have the same columns in the same order
    cols = df1.columns
    return df1.unionByName(df2.select(*cols))

df_with_sk = union_by_name(df_existing, df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    # Define the schema for the unknown member
    unknown_row = {
        "ShipperKey": -1,
        "shipperid": -1,
        "CompanyName": "Unknown",
        "Phone": "Unknown"
    }
    # Check if unknown member exists
    if df.filter(col("ShipperKey") == -1).count() == 0:
        unknown_df = spark.createDataFrame([unknown_row], schema=df.schema)
        df = df.unionByName(unknown_df)
    return df

df_with_sk = ensure_unknown_member(df_with_sk)

# 4. Mandatory Type Casting (STRICT)
# Define target schema columns and types explicitly
# (Assuming target schema: ShipperKey: INTEGER, shipperid: INTEGER, CompanyName: STRING, Phone: STRING)
target_schema = [
    {"name": "ShipperKey", "type": "integer"},
    {"name": "shipperid", "type": "integer"},
    {"name": "CompanyName", "type": "string"},
    {"name": "Phone", "type": "string"}
]

for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in df_with_sk.columns:
        df_with_sk = df_with_sk.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite/merge into DimShipper table (idempotent)
deltaTable = DeltaTable.forName(spark, target_table_name)
deltaTable.alias("target").merge(
    df_with_sk.alias("source"),
    "target.shipperid = source.shipperid"
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()

# Optimization: Z-ORDER on shipperid (high-cardinality business key)
spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (shipperid)")
