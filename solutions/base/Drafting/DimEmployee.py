# COMMAND ----------
# Title: DimEmployee.dtsx
# Auto-Generated by Shift-T Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window
import pyspark.sql.functions as F

# 2. Reading Bronze / Source
# -- JDBC connection details (replace with your actual secret scope/keys)
db_url = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_url")
db_user = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_user")
db_password = dbutils.secrets.get(scope="jdbc-secrets", key="hr_db_password")

# -- Parameter for empid > ? (simulate with a widget or default)
empid_min = dbutils.widgets.get("empid_min") if dbutils.widgets.get("empid_min", None) else "0"

sql_query = f"""
    SELECT empid, (firstname + ' ' + lastname) as fullname, title, city, country, address, phone
    FROM HR.Employees
    WHERE empid > {empid_min}
"""

# -- Read source data via JDBC
source_df = (
    spark.read.format("jdbc")
    .option("url", db_url)
    .option("user", db_user)
    .option("password", db_password)
    .option("dbtable", f"({sql_query}) as src")
    .option("driver", "com.microsoft.sqlserver.jdbc.SQLServerDriver")
    .load()
)

# 3. Transformations (Apply Logic)
# -- No lookups, just direct mapping
# -- Target columns: EmployeeKey (SK), EmployeeAlternateKey (empid), FullName, Title, City, Country, Address, Phone

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "DimEmployee"
bk_cols = ["empid"]  # Business Key is empid
sk_col = "EmployeeKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select("empid", sk_col)
    max_sk = df_target.agg(F.max(F.col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = source_df.join(df_target, on="empid", how="left")
else:
    df_joined = source_df.withColumn(sk_col, F.lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy("empid")
df_existing = df_joined.filter(F.col(sk_col).isNotNull())
df_new = df_joined.filter(F.col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, F.row_number().over(window_spec) + max_sk)

df_with_sk = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    unknown_row = {
        "EmployeeKey": -1,
        "empid": -1,
        "fullname": "Unknown",
        "title": "Unknown",
        "city": "Unknown",
        "country": "Unknown",
        "address": "Unknown",
        "phone": "Unknown"
    }
    # Check if -1 exists
    if df.filter(F.col("EmployeeKey") == -1).count() == 0:
        schema = df.schema
        unknown_df = spark.createDataFrame([unknown_row], schema=schema)
        return df.unionByName(unknown_df)
    else:
        return df

df_with_sk = ensure_unknown_member(df_with_sk)

# 4. Mandatory Type Casting (STRICT)
# -- Target schema (hardcoded, as not provided in input)
target_schema = [
    {"name": "EmployeeKey", "type": "INTEGER"},
    {"name": "empid", "type": "INTEGER"},
    {"name": "fullname", "type": "STRING"},
    {"name": "title", "type": "STRING"},
    {"name": "city", "type": "STRING"},
    {"name": "country", "type": "STRING"},
    {"name": "address", "type": "STRING"},
    {"name": "phone", "type": "STRING"}
]

# Rename columns to match target
rename_map = {
    "empid": "empid",
    "fullname": "fullname",
    "title": "title",
    "city": "city",
    "country": "country",
    "address": "address",
    "phone": "phone",
    "EmployeeKey": "EmployeeKey"
}
df_final = df_with_sk
for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in df_final.columns:
        df_final = df_final.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# -- Overwrite (idempotent) write
(
    df_final.select([f["name"] for f in target_schema])
    .write.format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(target_table_name)
)

# 6. Optimization (Z-ORDER on empid)
spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (empid)")
