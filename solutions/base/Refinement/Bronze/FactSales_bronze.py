# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: FactSales.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: FactSales.dtsx Migration
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Secure JDBC Connection Setup
# Use dbutils.secrets.get for credentials (replace with your secret scope and keys)
db_url = dbutils.secrets.get(scope="jdbc-secrets", key="dw-url")
db_user = dbutils.secrets.get(scope="jdbc-secrets", key="dw-user")
db_password = dbutils.secrets.get(scope="jdbc-secrets", key="dw-password")

# 3. Reading Source Data (Bronze Layer)
# Input 1: Orders + OrderDetails + Products
source_query_1 = '''select O.orderid,O.custid,O.empid,O.shipperid,P.categoryid,P.supplierid,OD.qty,OD.unitprice,OD.discount,OD.productid from Sales.orders O 
 INNER JOIN Sales.OrderDetails OD
 ON O.orderid =OD.orderid
 inner join Production.Products P
 ON P.productid=OD.productid'''
df_orders = spark.read.format("jdbc") \
    .option("url", db_url) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("dbtable", f"({source_query_1}) as src") \
    .load()

# Input 2: tempStage joined with V_DimProduct
source_query_2 = '''SELECT T.* 
FROM tempStage T 
INNER JOIN dw..V_DimProduct P 
ON P.productid = T.productid;'''
df_tempstage = spark.read.format("jdbc") \
    .option("url", db_url) \
    .option("user", db_user) \
    .option("password", db_password) \
    .option("dbtable", f"({source_query_2}) as src") \
    .load()

# 4. Transformations (Apply Logic)
# For demonstration, assume df_orders is staged to tempStage, then loaded to FactSales
# You may need to add business logic here as per requirements

# 5. Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
# For FactSales, generate Surrogate Key (e.g., FactSalesID)
target_table_name = "base.dbo.FactSales"
bk_cols = ["orderid", "productid"]  # Example business keys, adjust as per schema
sk_col = "FactSalesID"

try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except:
    df_target = None
    max_sk = 0

if df_target:
    df_joined = df_tempstage.join(df_target, on=bk_cols, how="left")
else:
    df_joined = df_tempstage.withColumn(sk_col, lit(None).cast("integer"))

window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)
df_with_sk = df_existing.unionByName(df_new)

# 6. Mandatory Type Casting (STRICT)
# You must cast every column to the target schema type
# Since the target schema is not provided, you must fetch it dynamically or define it here
# Example: (Replace with actual schema)
target_schema = [
    {"name": "FactSalesID", "type": "INTEGER"},
    {"name": "orderid", "type": "INTEGER"},
    {"name": "productid", "type": "INTEGER"},
    {"name": "custid", "type": "INTEGER"},
    {"name": "empid", "type": "INTEGER"},
    {"name": "shipperid", "type": "INTEGER"},
    {"name": "categoryid", "type": "INTEGER"},
    {"name": "supplierid", "type": "INTEGER"},
    {"name": "qty", "type": "INTEGER"},
    {"name": "unitprice", "type": "DECIMAL(19,4)"},
    {"name": "discount", "type": "DECIMAL(19,4)"}
]

for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in df_with_sk.columns:
        df_with_sk = df_with_sk.withColumn(col_name, col(col_name).cast(target_type))

# 7. Write to Target Table (Idempotent)
# Use Delta Lake MERGE for idempotency
# If FactSales is a fact table, use overwrite or merge as per business logic
factsales_delta = DeltaTable.forName(spark, target_table_name)
factsales_delta.alias("target").merge(
    df_with_sk.alias("stage"),
    "target.orderid = stage.orderid AND target.productid = stage.productid"
).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()

# 8. Z-ORDER Optimization (Optional, as per platform rules)
factsales_delta.optimize().zorderBy("orderid", "productid")

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.FactSales"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
