# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: MockComplexTable.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# We keep the source reading logic active to define 'df_source'
# MOCK for Testing Dynamic Key Detection
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
spark = SparkSession.builder.getOrCreate()

df = spark.read.table("source")

# This logic should trigger detection of composite keys ["Region", "EmpID"]
# because dropDuplicates is a strong uniqueness indicator.
df_dedup = df.dropDuplicates(["Region", "EmpID"])

# Some other logic
final_df = df_dedup.select("Region", "EmpID", "Sales")
# [DISABLED_BY_ARCHITECT] final_df.write.saveAsTable("MockComplexTable")


# Apply Bronze Standard
# NOTE: We assume 'df_source' is defined in the original code. 
# If the original code used 'df' or another name, we attempt to alias it.
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.MockComplexTable"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
