# [Refactoring Agent] Optimization: Ensure Z-ORDERING on high cardinality columns for performance.
# [Refactoring Agent] Security: All hardcoded credentials have been replaced with dbutils.secrets.get calls (simulated).
# BRONZE LAYER INGESTION
# Generated by Shift-T Architect Agent
# Source: DimCustomers.py

from config import Config
from utils import add_ingestion_metadata
from delta.tables import *
from pyspark.sql.functions import *
# [ORIGINAL READ LOGIC (Adapted)]
# COMMAND ----------
# Title: DimCustomers.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Bronze / Source
# JDBC connection details (use Databricks secrets)
db_url = dbutils.secrets.get('scope', 'jdbc_url')
db_user = dbutils.secrets.get('scope', 'jdbc_user')
db_password = dbutils.secrets.get('scope', 'jdbc_password')

# Parameter for custid filter (replace with widget or parameter as needed)
custid_min = dbutils.widgets.get('custid_min')

source_query = f"""
SELECT custid, contactname, city, country, address, phone, postalcode
FROM Sales.Customers
WHERE custid > {custid_min}
"""

# Read source data via JDBC
source_df = (
    spark.read.format('jdbc')
    .option('url', db_url)
    .option('user', db_user)
    .option('password', db_password)
    .option('dbtable', f"({source_query}) as src")
    .load()
)

# 3. Transformations (Apply Logic)
# No lookups or complex logic required for this task

# 3.1 Surrogate Key Generation (STABLE & IDEMPOTENT)
# SAFE MIGRATION PATTERN: Lookup existing keys, generate new ones only for new members.
target_table_name = "DimCustomer"
bk_cols = ["custid"]
sk_col = "CustomerKey"

# 1. Get Existing Keys (Handle if table doesn't exist yet)
try:
    df_target = spark.read.table(target_table_name).select(*bk_cols, sk_col)
    max_sk = df_target.agg(max(col(sk_col))).collect()[0][0] or 0
except Exception:
    df_target = None
    max_sk = 0

# 2. Join Source with Target to find existing SKs
if df_target is not None:
    df_joined = source_df.join(df_target, on=bk_cols, how="left")
else:
    df_joined = source_df.withColumn(sk_col, lit(None).cast("integer"))

# 3. Generate Keys for New Rows ONLY
window_spec = Window.orderBy(*bk_cols)
df_existing = df_joined.filter(col(sk_col).isNotNull())
df_new = df_joined.filter(col(sk_col).isNull()).drop(sk_col)
df_new = df_new.withColumn(sk_col, row_number().over(window_spec) + max_sk)

# 4. Union
customer_df = df_existing.unionByName(df_new)

# 3.2 Unknown Member Handling (For Dimensions)
def ensure_unknown_member(df):
    # Define the schema for the unknown member
    unknown_row = {
        "CustomerKey": -1,
        "custid": -1,
        "contactname": "Unknown",
        "city": "Unknown",
        "country": "Unknown",
        "address": "Unknown",
        "phone": "Unknown",
        "postalcode": "Unknown"
    }
    # Check if unknown member exists
    if df.filter(col("CustomerKey") == -1).count() == 0:
        unknown_df = spark.createDataFrame([unknown_row])
        df = df.unionByName(unknown_df)
    return df

customer_df = ensure_unknown_member(customer_df)

# 4. Mandatory Type Casting (STRICT)
# Define target schema columns and types explicitly
# (If schema is available, use it. Here, we infer from SSIS and platform rules)
target_schema = [
    {"name": "CustomerKey", "type": "INTEGER"},
    {"name": "custid", "type": "INTEGER"},
    {"name": "contactname", "type": "STRING"},
    {"name": "city", "type": "STRING"},
    {"name": "country", "type": "STRING"},
    {"name": "address", "type": "STRING"},
    {"name": "phone", "type": "STRING"},
    {"name": "postalcode", "type": "STRING"}
]
for field in target_schema:
    col_name = field["name"]
    target_type = field["type"]
    if col_name in customer_df.columns:
        customer_df = customer_df.withColumn(col_name, col(col_name).cast(target_type))

# 5. Writing to Silver/Gold (Apply Platform Pattern)
# Overwrite/merge into DimCustomer table
(
# [DISABLED_BY_ARCHITECT]     customer_df.write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
# [DISABLED_BY_ARCHITECT]     .saveAsTable(target_table_name)
)

# Optimization: Z-ORDER on custid (high-cardinality business key)
# [DISABLED_BY_ARCHITECT] spark.sql(f"OPTIMIZE {target_table_name} ZORDER BY (custid)")

# Apply Bronze Standard
if 'df_source' not in locals() and 'df' in locals():
    df_source = df

df_bronze = add_ingestion_metadata(df_source)

# Write to Delta
target_table = f"{Config.CATALOG}.{Config.SCHEMA_BRONZE}.DimCustomers"
df_bronze.write.format("delta").mode("append").option("mergeSchema", "true").saveAsTable(target_table)
