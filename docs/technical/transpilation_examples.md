# Ejemplo de Transpilación: Input vs Output

Este documento ilustra la capacidad de interpretación del motor "Shift-T". Se muestra el fragmento original de un paquete SSIS (Input) y el código PySpark generado automáticamente por el sistema (Output).

## 1. Input: El Legado (SSIS/XML)

El sistema ingiere archivos `.dtsx`. A continuación se muestra un fragmento real de `DimCategory.dtsx`. Observa cómo la lógica SQL está enterrada dentro de propiedades XML y estructuras propietarias de Microsoft.

**Archivo:** `DimCategory.dtsx` (Fragmento)

```xml
<DTS:Executable
  DTS:refId="Package\DimCategory"
  DTS:CreationName="Microsoft.Pipeline"
  DTS:Description="Data Flow Task">
  
  <DTS:ObjectData>
    <pipeline version="1">
      <components>
        <!-- OLE DB SOURCE: Extracción de Datos -->
        <component
          refId="Package\DimCategory\OLE DB Source"
          name="OLE DB Source"
          componentClassID="Microsoft.OLEDBSource">
          
          <properties>
            <!-- Lógica SQL embebida en la propiedad SqlCommand -->
            <property name="SqlCommand" dataType="System.String">
              SELECT categoryid, categoryname FROM Production.Categories
              WHERE categoryid > ?
            </property>
            <property name="ParameterMapping">"0:Input",{98227627-...};</property>
          </properties>
          
          <outputs>
             <output name="OLE DB Source Output">
               <outputColumns>
                 <outputColumn name="categoryid" dataType="i4" />
                 <outputColumn name="categoryname" dataType="wstr" length="15" />
               </outputColumns>
             </output>
          </outputs>
        </component>

        <!-- OLE DB DESTINATION: Carga -->
        <component
          refId="Package\DimCategory\OLE DB Destination"
          name="OLE DB Destination">
          <properties>
             <property name="OpenRowset">[DimCategory]</property>
             <property name="FastLoadOptions">TABLOCK,CHECK_CONSTRAINTS</property>
          </properties>
        </component>
      </components>
    </pipeline>
  </DTS:ObjectData>
</DTS:Executable>
```

**Análisis del Input:**
*   Formato ilegible para humanos (XML verboso).
*   Lógica difusa: La query SQL está mezclada con configuración de la UI (`OpenRowset`, `FastLoadOptions`).
*   Parámetros ocultos (`?`) mapeados por IDs internos (`{98227627...}`).

---

## 2. Output: El Futuro (PySpark Moderno)

El Agente C interpreta no solo el SQL, sino la *intención* del paquete (ETL de Dimensiones), y genera código Python optimizado para Databricks/Spark, aplicando patrones modernos como Claves Subrogadas y Manejo de Miembros Desconocidos.

**Archivo Generado:** `DimCategory.py`

```python
# COMMAND ----------
# Title: DimCategory.dtsx
# Auto-Generated by Platform Developer Agent
# --------------------------------------------------

# 1. Setup & Config
from delta.tables import *
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql.window import Window

# 2. Reading Bronze / Source
# Logic extracted from "OLE DB Source" -> SqlCommand
# Parameter '?' converted to Databricks Widget/Variable
categoryid_min = int(dbutils.widgets.get('categoryid_min', '-1'))

# Secure Connection (Secrets Management instead of connection strings)
jdbc_url = dbutils.secrets.get('jdbc', 'sqlserver_url')
jdbc_user = dbutils.secrets.get('jdbc', 'sqlserver_user')
jdbc_password = dbutils.secrets.get('jdbc', 'sqlserver_password')

source_query = f"""
SELECT categoryid, categoryname FROM Production.Categories WHERE categoryid > {categoryid_min}
"""

source_df = (
    spark.read.format('jdbc')
    .option('url', jdbc_url)
    .option('user', jdbc_user)
    .option('password', jdbc_password)
    .option('dbtable', f"({source_query}) as src")
    .load()
)

# 3. Transformations (The "Shift-T" Logic Upgrade)
# The agent identifies this is a Dimension Load and auto-injects Surrogate Key logic
# which wasn't even explicit in the original simple INSERT!

target_table_name = "base.dbo.DimCategory"
bk_cols = ["categoryid"]
sk_col = "CategoryKey"

# ... (Surrogate Key Generation Logic omitted for brevity) ...

# 4. Mandatory Type Casting (Target Schema Enforcement)
# Derived from "OLE DB Destination" -> ExternalMetadataColumns
staged_df = staged_df.withColumn("CategoryKey", col("CategoryKey").cast("integer"))
staged_df = staged_df.withColumn("categoryid", col("categoryid").cast("integer"))
staged_df = staged_df.withColumn("categoryname", col("categoryname").cast("string"))

# 5. Writing to Silver/Gold (Delta Lake)
# Replaces "FastLoadOptions" with Delta Overwrite schema evolution
(
    staged_df
    .write
    .format("delta")
    .mode("overwrite")
    .option("overwriteSchema", "true")
    .saveAsTable(target_table_name)
)

# 6. Optimization
# Auto-added Z-ORDER optimization which didn't exist in SSIS
deltaTable = DeltaTable.forName(spark, target_table_name)
deltaTable.optimize().executeZOrderBy("categoryid")
```

**Análisis del Output:**
*   **Código Limpio**: Python estandarizado, fácil de leer y versionar en Git.
*   **Enriquecimiento Inteligente**: El agente no solo tradujo, sino que *mejoró* el proceso añadiendo `Z-ORDER`, manejo de secretos (`dbutils.secrets`) y widgets.
*   **Patrones de Ingeniería de Datos**: Detectó que era una "Dimensión" e inyectó lógica de Claves Subrogadas (`CategoryKey`) que es best-practice en Lakehouse, aunque no estaba explícita en el simple flujo de SSIS.
